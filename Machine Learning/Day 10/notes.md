# Day 10 – Machine Learning  

###  What I Learned Today  
- Completed understanding of **Linear Regression**  
  - Trained & tested models  
  - Understood predictions and error evaluation  
- Started exploring **Multivariable Regression** (multiple features instead of one)  

---

###  Key Concepts  
1. **Linear Regression Recap**  
   - Hypothesis: `y = m*x + c`  
   - Cost Function: Mean Squared Error (MSE)  
   - Optimization: Gradient Descent  

2. **Multivariable Regression**  
   - Hypothesis becomes:  
     `y = w1*x1 + w2*x2 + ... + wn*xn + b`  
   - Multiple features → model learns **different weights** for each feature  
   - Still optimized with **Gradient Descent**  

---

###  Reflection  
- Completing Linear Regression gave me confidence in the **core ML workflow**  
- Transitioning to **Multivariable Regression** shows how ML scales to real-world problems  
- Excited to apply it on datasets with multiple features and see the difference in predictions  
